#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –®—Ä–∏–¥—Ö–∞—Ä–∞ –ú–∞—Ö–∞—Ä–∞–¥–∂–∞
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç shridhar_maharaj_books –∏ mozgach_alice_gift_sql_dataset
"""

import os
import json
import sqlite3
import re
from pathlib import Path
from datasets import Dataset, DatasetDict
import pandas as pd
from typing import List, Dict, Any

def clean_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤"""
    # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
    text = re.sub(r'\s+', ' ', text)
    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
    text = text.strip()
    return text

def load_shridhar_books(dataset_path: str) -> List[Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–Ω–∏–≥ –®—Ä–∏–¥—Ö–∞—Ä–∞ –ú–∞—Ö–∞—Ä–∞–¥–∂–∞"""
    books = []
    txt_files = list(Path(dataset_path).glob("*.txt"))
    
    for txt_file in txt_files:
        print(f"–ó–∞–≥—Ä—É–∂–∞—é –∫–Ω–∏–≥—É: {txt_file.name}")
        
        try:
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            cleaned_content = clean_text(content)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ
            lines = content.split('\n')
            title = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–Ω–∏–≥–∞"
            
            for i, line in enumerate(lines[:20]):
                line = line.strip()
                if len(line) > 10 and len(line) < 100 and not line.startswith(' '):
                    if not any(skip in line.lower() for skip in ['—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ', '–≥–ª–∞–≤–∞', '–æ—Ç –∏–∑–¥–∞—Ç–µ–ª–µ–π', '–ø—Ä–µ–¥–∏—Å–ª–æ–≤–∏–µ']):
                        title = line
                        break
            
            book_record = {
                'id': f"shridhar_{txt_file.stem}",
                'text': cleaned_content,
                'source': 'shridhar_books',
                'title': title,
                'author': '–®—Ä–∏–ª–∞ –ë—Ö–∞–∫—Ç–∏ –†–∞–∫—à–∞–∫ –®—Ä–∏–¥—Ö–∞—Ä –î–µ–≤-–ì–æ—Å–≤–∞–º–∏ –ú–∞—Ö–∞—Ä–∞–¥–∂',
                'language': 'ru',
                'topic': 'spirituality',
                'religion': 'vaishnavism',
                'text_length': len(cleaned_content),
                'word_count': len(cleaned_content.split()),
                'context_type': 'spiritual_text'
            }
            
            books.append(book_record)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {txt_file.name}: {e}")
            continue
    
    return books

def load_alice_sql_data(dataset_path: str) -> List[Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ SQL –¥–∞–º–ø–∞ Alice"""
    dialogues = []
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é SQLite –±–∞–∑—É
    temp_db = "/tmp/alice_temp.db"
    
    try:
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —á–∞—Å—Ç–∏ SQL –¥–∞–º–ø–∞
        print("–û–±—ä–µ–¥–∏–Ω—è—é SQL –¥–∞–º–ø...")
        sql_files = sorted(Path(dataset_path).glob("alice_*"))
        
        with open("/tmp/alice_combined.sql", 'w', encoding='utf-8') as outfile:
            for sql_file in sql_files:
                print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {sql_file.name}")
                with open(sql_file, 'r', encoding='utf-8') as infile:
                    content = infile.read()
                    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –∏ —Ñ–æ—Ä–º–∞—Ç
                    content = content.replace('utf8mb4', 'utf8')
                    content = content.replace('utf8mb3', 'utf8')
                    outfile.write(content)
        
        print("–ò–º–ø–æ—Ä—Ç–∏—Ä—É—é –≤ SQLite...")
        # –°–æ–∑–¥–∞–µ–º SQLite —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
        conn = sqlite3.connect(temp_db)
        cursor = conn.cursor()
        
        # –ß–∏—Ç–∞–µ–º –∏ –≤—ã–ø–æ–ª–Ω—è–µ–º SQL
        with open("/tmp/alice_combined.sql", 'r', encoding='utf-8') as f:
            sql_content = f.read()
            
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        sql_commands = sql_content.split(';')
        
        for i, command in enumerate(sql_commands):
            if command.strip():
                try:
                    cursor.execute(command)
                    if i % 100 == 0:
                        print(f"–í—ã–ø–æ–ª–Ω–µ–Ω–æ –∫–æ–º–∞–Ω–¥: {i}")
                except Exception as e:
                    if "table" not in str(e).lower():
                        print(f"–û—à–∏–±–∫–∞ –≤ –∫–æ–º–∞–Ω–¥–µ {i}: {e}")
                    continue
        
        conn.commit()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∏–∞–ª–æ–≥–∏
        print("–ò–∑–≤–ª–µ–∫–∞—é –¥–∏–∞–ª–æ–≥–∏...")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        try:
            cursor.execute("SELECT * FROM messages LIMIT 10000")
            messages = cursor.fetchall()
            
            for msg in messages:
                if len(msg) >= 3 and msg[2]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ–∫—Å—Ç–∞
                    text = str(msg[2]).strip()
                    if len(text) > 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
                        dialogue_record = {
                            'id': f"alice_msg_{msg[0]}",
                            'text': clean_text(text),
                            'source': 'alice_dialogues',
                            'title': f"–î–∏–∞–ª–æ–≥ {msg[0]}",
                            'author': 'Alice Bot',
                            'language': 'ru',
                            'topic': 'dialogue',
                            'religion': 'none',
                            'text_length': len(text),
                            'word_count': len(text.split()),
                            'context_type': 'dialogue'
                        }
                        dialogues.append(dialogue_record)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π: {e}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏
        try:
            cursor.execute("SELECT * FROM scripts LIMIT 5000")
            scripts = cursor.fetchall()
            
            for script in scripts:
                if len(script) >= 2 and script[1]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Ü–µ–Ω–∞—Ä–∏—è
                    text = str(script[1]).strip()
                    if len(text) > 10:
                        script_record = {
                            'id': f"alice_script_{script[0]}",
                            'text': clean_text(text),
                            'source': 'alice_scripts',
                            'title': f"–°—Ü–µ–Ω–∞—Ä–∏–π {script[0]}",
                            'author': 'Alice Bot',
                            'language': 'ru',
                            'topic': 'dialogue',
                            'religion': 'none',
                            'text_length': len(text),
                            'word_count': len(text.split()),
                            'context_type': 'script'
                        }
                        dialogues.append(script_record)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤: {e}")
        
        conn.close()
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ SQL –¥–∞–Ω–Ω—ã—Ö: {e}")
    finally:
        # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        if os.path.exists(temp_db):
            os.remove(temp_db)
        if os.path.exists("/tmp/alice_combined.sql"):
            os.remove("/tmp/alice_combined.sql")
    
    return dialogues

def create_combined_dataset(shridhar_books: List[Dict], alice_data: List[Dict]) -> DatasetDict:
    """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
    all_data = shridhar_books + alice_data
    
    print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(all_data)}")
    print(f"–ö–Ω–∏–≥–∏ –®—Ä–∏–¥—Ö–∞—Ä–∞: {len(shridhar_books)}")
    print(f"–î–∞–Ω–Ω—ã–µ Alice: {len(alice_data)}")
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/validation/test (80/10/10)
    total_size = len(all_data)
    train_size = int(0.8 * total_size)
    val_size = int(0.1 * total_size)
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    import random
    random.shuffle(all_data)
    
    train_data = all_data[:train_size]
    val_data = all_data[train_size:train_size + val_size]
    test_data = all_data[train_size + val_size:]
    
    # –°–æ–∑–¥–∞–µ–º DatasetDict
    dataset_dict = DatasetDict({
        'train': Dataset.from_list(train_data),
        'validation': Dataset.from_list(val_data),
        'test': Dataset.from_list(test_data)
    })
    
    return dataset_dict

def create_tokenizer_config():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
    tokenizer_config = {
        "vocab_size": 50000,
        "model_max_length": 8192,  # 8K –∫–æ–Ω—Ç–µ–∫—Å—Ç
        "special_tokens": [
            "<|spiritual|>",
            "<|dialogue|>", 
            "<|script|>",
            "<|end|>",
            "<|pad|>",
            "<|unk|>"
        ],
        "language": "ru",
        "tokenizer_type": "BPE",
        "lowercase": False,
        "strip_accents": False
    }
    return tokenizer_config

def create_training_config():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 8K –º–æ–¥–µ–ª–∏"""
    training_config = {
        "model_config": {
            "vocab_size": 50000,
            "n_positions": 8192,      # 8K –∫–æ–Ω—Ç–µ–∫—Å—Ç
            "n_embd": 768,            # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            "n_layer": 12,           # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤
            "n_head": 12,             # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
            "attn_pdrop": 0.1,
            "embd_pdrop": 0.1,
            "resid_pdrop": 0.1,
            "activation_function": "gelu_new"
        },
        "training_args": {
            "per_device_train_batch_size": 1,      # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π batch –¥–ª—è 8K
            "per_device_eval_batch_size": 1,
            "gradient_accumulation_steps": 16,     # –ò–º–∏—Ç–∞—Ü–∏—è batch_size=16
            "learning_rate": 1e-4,
            "num_train_epochs": 3,
            "max_steps": 10000,
            "warmup_steps": 500,
            "logging_steps": 100,
            "save_steps": 500,
            "eval_steps": 500,
            "evaluation_strategy": "steps",
            "save_strategy": "steps",
            "load_best_model_at_end": True,
            "metric_for_best_model": "eval_loss",
            "greater_is_better": False,
            "fp16": True,
            "gradient_checkpointing": True,
            "dataloader_pin_memory": True,
            "dataloader_num_workers": 4,
            "remove_unused_columns": False,
            "report_to": "tensorboard",
            "run_name": "shridhar_8k_training"
        },
        "optimization": {
            "use_mps": True,           # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU M4
            "mixed_precision": "fp16",
            "gradient_checkpointing": True,
            "flash_attention": True,
            "memory_efficient_attention": True
        }
    }
    return training_config

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    
    print("üöÄ –ù–∞—á–∏–Ω–∞—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 8K –º–æ–¥–µ–ª–∏...")
    
    # –ü—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º
    shridhar_path = "/Users/anton/proj/ai.nativemind.net/sridhar/datasets/shridhar_maharaj_books"
    alice_path = "/Users/anton/proj/ai.nativemind.net/sridhar/datasets/mozgach_alice_gift_sql_dataset"
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –®—Ä–∏–¥—Ö–∞—Ä–∞ –ú–∞—Ö–∞—Ä–∞–¥–∂–∞
    print("\nüìö –ó–∞–≥—Ä—É–∂–∞—é –∫–Ω–∏–≥–∏ –®—Ä–∏–¥—Ö–∞—Ä–∞ –ú–∞—Ö–∞—Ä–∞–¥–∂–∞...")
    shridhar_books = load_shridhar_books(shridhar_path)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(shridhar_books)} –∫–Ω–∏–≥")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ Alice
    print("\nü§ñ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ Alice...")
    alice_data = load_alice_sql_data(alice_path)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(alice_data)} –¥–∏–∞–ª–æ–≥–æ–≤/—Å—Ü–µ–Ω–∞—Ä–∏–µ–≤")
    
    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    print("\nüîÑ –°–æ–∑–¥–∞—é –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç...")
    dataset_dict = create_combined_dataset(shridhar_books, alice_data)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    output_path = "/Users/anton/proj/ai.nativemind.net/sridhar/combined_shridhar_alice_dataset"
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é –¥–∞—Ç–∞—Å–µ—Ç –≤: {output_path}")
    dataset_dict.save_to_disk(output_path)
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    print("\n‚öôÔ∏è –°–æ–∑–¥–∞—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏...")
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    tokenizer_config = create_tokenizer_config()
    with open(f"{output_path}/tokenizer_config.json", 'w', encoding='utf-8') as f:
        json.dump(tokenizer_config, f, ensure_ascii=False, indent=2)
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
    training_config = create_training_config()
    with open(f"{output_path}/training_config.json", 'w', encoding='utf-8') as f:
        json.dump(training_config, f, ensure_ascii=False, indent=2)
    
    # –°–æ–∑–¥–∞–µ–º README
    readme_content = f"""# –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –®—Ä–∏–¥—Ö–∞—Ä–∞ –ú–∞—Ö–∞—Ä–∞–¥–∂–∞

## –û–ø–∏—Å–∞–Ω–∏–µ
–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç:
- {len(shridhar_books)} –∫–Ω–∏–≥ –®—Ä–∏–¥—Ö–∞—Ä–∞ –ú–∞—Ö–∞—Ä–∞–¥–∂–∞ (–¥—É—Ö–æ–≤–Ω–∞—è —Ç–µ–º–∞—Ç–∏–∫–∞)
- {len(alice_data)} –¥–∏–∞–ª–æ–≥–æ–≤/—Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ Alice (—Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–∞—è —Ç–µ–º–∞—Ç–∏–∫–∞)

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞
- **Train**: {len(dataset_dict['train'])} –∑–∞–ø–∏—Å–µ–π
- **Validation**: {len(dataset_dict['validation'])} –∑–∞–ø–∏—Å–µ–π  
- **Test**: {len(dataset_dict['test'])} –∑–∞–ø–∏—Å–µ–π

## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞**: 8,192 —Ç–æ–∫–µ–Ω–æ–≤
- **–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è**: 50,000 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: GPT-–ø–æ–¥–æ–±–Ω–∞—è –º–æ–¥–µ–ª—å
- **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**: MPS, FP16, Gradient Checkpointing

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
```python
from datasets import load_from_disk

dataset = load_from_disk("{output_path}")
```
"""
    
    with open(f"{output_path}/README.md", 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_text_length = sum(item['text_length'] for item in shridhar_books + alice_data)
    total_words = sum(item['word_count'] for item in shridhar_books + alice_data)
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
    print(f"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(shridhar_books + alice_data)}")
    print(f"   –ö–Ω–∏–≥–∏ –®—Ä–∏–¥—Ö–∞—Ä–∞: {len(shridhar_books)}")
    print(f"   –î–∞–Ω–Ω—ã–µ Alice: {len(alice_data)}")
    print(f"   –û–±—â–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {total_text_length:,} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {total_words:,}")
    print(f"   Train: {len(dataset_dict['train'])} –∑–∞–ø–∏—Å–µ–π")
    print(f"   Validation: {len(dataset_dict['validation'])} –∑–∞–ø–∏—Å–µ–π")
    print(f"   Test: {len(dataset_dict['test'])} –∑–∞–ø–∏—Å–µ–π")
    
    print(f"\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω!")
    print(f"üìÅ –ü—É—Ç—å: {output_path}")
    print(f"üéØ –ì–æ—Ç–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–∏ —Å 8K –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º")

if __name__ == "__main__":
    main()
