#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ shridhar_8k –≤ GGUF —Ñ–æ—Ä–º–∞—Ç
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import subprocess
import sys

def install_llama_cpp():
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ llama.cpp –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ GGUF"""
    print("üì¶ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é llama.cpp...")
    try:
        subprocess.run([sys.executable, "-m", "pip", "install", "llama-cpp-python"], check=True)
        print("‚úÖ llama.cpp —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except subprocess.CalledProcessError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ llama.cpp: {e}")
        return False
    return True

def load_and_merge_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏"""
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞—é –∏ –æ–±—ä–µ–¥–∏–Ω—è—é –º–æ–¥–µ–ª—å...")
    
    base_model_name = "nativemind/braindler_full_trained_model"
    lora_path = "./braindler_finetuned_8k"
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
        print("–ó–∞–≥—Ä—É–∂–∞—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="cpu"
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã
        print("–ó–∞–≥—Ä—É–∂–∞—é LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã...")
        model = PeftModel.from_pretrained(base_model, lora_path)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º LoRA —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
        print("–û–±—ä–µ–¥–∏–Ω—è—é LoRA —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é...")
        merged_model = model.merge_and_unload()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        output_dir = "./shridhar_8k_merged"
        print(f"–°–æ—Ö—Ä–∞–Ω—è—é –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ {output_dir}...")
        merged_model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        print("‚úÖ –ú–æ–¥–µ–ª—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
        return output_dir
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        return None

def convert_to_gguf(model_path):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ GGUF —Ñ–æ—Ä–º–∞—Ç"""
    print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é –º–æ–¥–µ–ª—å –≤ GGUF —Ñ–æ—Ä–º–∞—Ç...")
    
    try:
        from llama_cpp import Llama
        
        # –°–æ–∑–¥–∞–µ–º GGUF —Ñ–∞–π–ª
        gguf_path = "./shridhar_8k.gguf"
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º llama.cpp –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
        print("–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é –≤ GGUF...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ transformers
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="cpu"
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–º —Å llama.cpp
        print("–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—é –º–æ–¥–µ–ª—å –¥–ª—è GGUF...")
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è GGUF
        config = {
            "model_type": "gpt2",
            "vocab_size": model.config.vocab_size,
            "n_positions": 8192,  # 8K –∫–æ–Ω—Ç–µ–∫—Å—Ç
            "n_embd": model.config.n_embd,
            "n_layer": model.config.n_layer,
            "n_head": model.config.n_head,
            "activation_function": model.config.activation_function,
            "bos_token_id": model.config.bos_token_id,
            "eos_token_id": model.config.eos_token_id,
            "pad_token_id": model.config.pad_token_id
        }
        
        print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GGUF —Å–æ–∑–¥–∞–Ω–∞")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ GGUF: {e}")
        return False

def create_gguf_script():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ GGUF"""
    script_content = '''#!/bin/bash
# –°–∫—Ä–∏–ø—Ç –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –≤ GGUF —Ñ–æ—Ä–º–∞—Ç

echo "üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é –º–æ–¥–µ–ª—å shridhar_8k –≤ GGUF..."

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install llama-cpp-python

# –°–∫–∞—á–∏–≤–∞–µ–º llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º
make

# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
python convert_hf_to_gguf.py ../../shridhar_8k_merged --outfile ../../shridhar_8k.gguf --outtype f16

echo "‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!"
'''
    
    with open("./convert_gguf.sh", "w") as f:
        f.write(script_content)
    
    os.chmod("./convert_gguf.sh", 0o755)
    print("‚úÖ –°–∫—Ä–∏–ø—Ç –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω: convert_gguf.sh")

def create_gguf_usage_script():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GGUF –º–æ–¥–µ–ª–∏"""
    usage_script = '''#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GGUF –º–æ–¥–µ–ª–∏ shridhar_8k
"""

from llama_cpp import Llama

def load_gguf_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ GGUF –º–æ–¥–µ–ª–∏"""
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞—é GGUF –º–æ–¥–µ–ª—å...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    llm = Llama(
        model_path="./shridhar_8k.gguf",
        n_ctx=8192,  # 8K –∫–æ–Ω—Ç–µ–∫—Å—Ç
        n_threads=4,
        verbose=False
    )
    
    print("‚úÖ GGUF –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    return llm

def generate_text(llm, prompt, max_tokens=300):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é GGUF –º–æ–¥–µ–ª–∏"""
    print(f"üìù –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞: {prompt}")
    
    response = llm(
        prompt,
        max_tokens=max_tokens,
        temperature=0.7,
        top_p=0.9,
        repeat_penalty=1.1,
        stop=["</s>", "\n\n"]
    )
    
    return response['choices'][0]['text']

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GGUF –º–æ–¥–µ–ª–∏ shridhar_8k")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    llm = load_gguf_model()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã
    test_prompts = [
        "–†–∞—Å—Å–∫–∞–∂–∏ –æ —Ñ–∏–ª–æ—Å–æ—Ñ–∏–∏ –≤–∞–π—à–Ω–∞–≤–∏–∑–º–∞:",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –±—Ö–∞–∫—Ç–∏-–π–æ–≥–∞?",
        "–û–±—ä—è—Å–Ω–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ö—Ä–∏—à–Ω—ã:",
        "–ö–∞–∫–æ–≤–∞ —Ä–æ–ª—å –≥—É—Ä—É –≤ –¥—É—Ö–æ–≤–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–µ?"
    ]
    
    for prompt in test_prompts:
        print(f"\\n{'='*60}")
        print(f"üìù –ü—Ä–æ–º–ø—Ç: {prompt}")
        print(f"{'='*60}")
        
        response = generate_text(llm, prompt)
        print(f"ü§ñ –û—Ç–≤–µ—Ç: {response}")

if __name__ == "__main__":
    main()
'''
    
    with open("./use_gguf_model.py", "w") as f:
        f.write(usage_script)
    
    print("‚úÖ –°–∫—Ä–∏–ø—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GGUF —Å–æ–∑–¥–∞–Ω: use_gguf_model.py")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏"""
    print("üöÄ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ shridhar_8k –≤ GGUF —Ñ–æ—Ä–º–∞—Ç")
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º llama.cpp
    if not install_llama_cpp():
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_path = load_and_merge_model()
    if not model_path:
        return
    
    # –°–æ–∑–¥–∞–µ–º —Å–∫—Ä–∏–ø—Ç—ã –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
    create_gguf_script()
    create_gguf_usage_script()
    
    print("\\nüìã –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ GGUF:")
    print("1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: bash convert_gguf.sh")
    print("2. –ü–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: python use_gguf_model.py")
    print("\\n‚úÖ –í—Å–µ —Å–∫—Ä–∏–ø—Ç—ã –≥–æ—Ç–æ–≤—ã –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ GGUF!")

if __name__ == "__main__":
    main()
