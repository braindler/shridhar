#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –®—Ä–∏–¥—Ö–∞—Ä–∞ –ú–∞—Ö–∞—Ä–∞–¥–∂–∞ —Å 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –Ω–∞ MacBook Pro M4
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
"""

import os
import json
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    GPT2Config,
    GPT2LMHeadModel
)
from datasets import load_from_disk
import numpy as np
from typing import Dict, List, Any
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ShridharModelConfig18K:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self):
        self.vocab_size = 50257  # GPT-2 —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
        self.n_positions = 18432  # 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç
        self.n_embd = 768
        self.n_layer = 12
        self.n_head = 12
        self.attn_pdrop = 0.1
        self.embd_pdrop = 0.1
        self.resid_pdrop = 0.1
        self.activation_function = "gelu_new"
        self.bos_token_id = 0
        self.eos_token_id = 1
        self.pad_token_id = 2

class ShridharDataset18K:
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
    
    def __init__(self, dataset_path: str, tokenizer, max_length: int = 18432):
        self.dataset = load_from_disk(dataset_path)
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.dataset['train'])
    
    def __getitem__(self, idx):
        item = self.dataset['train'][idx]
        text = item['text']
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if item.get('context_type') == 'spiritual_text':
            text = f"<|spiritual|>{text}<|end|>"
        elif item.get('context_type') == 'dialogue':
            text = f"<|dialogue|>{text}<|end|>"
        elif item.get('context_type') == 'script':
            text = f"<|script|>{text}<|end|>"
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å –ø–æ–º–æ—â—å—é GPT-2 —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        tokens = self.tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=self.max_length)
        
        return {
            'input_ids': torch.tensor(tokens, dtype=torch.long),
            'labels': torch.tensor(tokens, dtype=torch.long)
        }

def create_model_18k(config: ShridharModelConfig18K) -> GPT2LMHeadModel:
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ GPT-2 –¥–ª—è 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    
    model_config = GPT2Config(
        vocab_size=config.vocab_size,
        n_positions=config.n_positions,
        n_embd=config.n_embd,
        n_layer=config.n_layer,
        n_head=config.n_head,
        attn_pdrop=config.attn_pdrop,
        embd_pdrop=config.embd_pdrop,
        resid_pdrop=config.resid_pdrop,
        activation_function=config.activation_function,
        bos_token_id=config.bos_token_id,
        eos_token_id=config.eos_token_id,
        pad_token_id=config.pad_token_id
    )
    
    model = GPT2LMHeadModel(model_config)
    return model

def setup_training_environment_18k():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ä–µ–¥—ã –æ–±—É—á–µ–Ω–∏—è –¥–ª—è M4 —Å 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å MPS
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è MPS (Metal Performance Shaders)")
    else:
        device = torch.device("cpu")
        logger.warning("‚ö†Ô∏è MPS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
    if device.type == "mps":
        # –î–ª—è MPS –Ω–µ—Ç –ø—Ä—è–º–æ–≥–æ –∞–Ω–∞–ª–æ–≥–∞ empty_cache, –Ω–æ –º–æ–∂–Ω–æ –æ—á–∏—Å—Ç–∏—Ç—å –∫—ç—à Python
        import gc
        gc.collect()
        logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
    
    return device

def create_training_arguments_18k():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    
    return TrainingArguments(
        output_dir="./shridhar_18k_model",
        per_device_train_batch_size=1,        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π batch –¥–ª—è 18K
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=32,       # –ò–º–∏—Ç–∞—Ü–∏—è batch_size=32
        learning_rate=5e-5,                   # –ú–µ–Ω—å—à–∏–π learning rate –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        num_train_epochs=3,
        max_steps=15000,                      # –ë–æ–ª—å—à–µ —à–∞–≥–æ–≤ –¥–ª—è 18K
        warmup_steps=1000,                    # –ë–æ–ª—å—à–µ warmup
        logging_steps=200,                    # –†–µ–∂–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        save_steps=1000,                      # –†–µ–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        eval_steps=1000,
        eval_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        fp16=False,                          # MPS –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç fp16
        gradient_checkpointing=True,         # –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        dataloader_pin_memory=False,         # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        dataloader_num_workers=2,            # –ú–µ–Ω—å—à–µ –≤–æ—Ä–∫–µ—Ä–æ–≤
        remove_unused_columns=False,
        report_to="tensorboard",
        run_name="shridhar_18k_training",
        save_total_limit=2,                 # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–π
        prediction_loss_only=True,
        dataloader_drop_last=True,
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è 18K
        dataloader_prefetch_factor=2,        # –ú–µ–Ω—å—à–µ prefetch
        max_grad_norm=1.0,                  # Gradient clipping
        weight_decay=0.01,                  # Weight decay
        adam_beta1=0.9,
        adam_beta2=0.95,
        adam_epsilon=1e-8
    )

def monitor_memory_usage():
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    import psutil
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏
    memory = psutil.virtual_memory()
    logger.info(f"üíæ –ü–∞–º—è—Ç—å: {memory.used / (1024**3):.1f}GB / {memory.total / (1024**3):.1f}GB ({memory.percent:.1f}%)")
    
    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
    if memory.percent > 90:
        logger.warning("‚ö†Ô∏è –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏! –í–æ–∑–º–æ–∂–µ–Ω OOM.")
    elif memory.percent > 80:
        logger.warning("‚ö†Ô∏è –£–º–µ—Ä–µ–Ω–Ω–æ –≤—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏.")
    else:
        logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ –Ω–æ—Ä–º–µ.")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
    
    logger.info("üöÄ –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –®—Ä–∏–¥—Ö–∞—Ä–∞ –ú–∞—Ö–∞—Ä–∞–¥–∂–∞ —Å 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º...")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ä–µ–¥—ã
    device = setup_training_environment_18k()
    logger.info(f"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏
    monitor_memory_usage()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
    model_config = ShridharModelConfig18K()
    logger.info(f"üìê –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏: {model_config.n_positions} —Ç–æ–∫–µ–Ω–æ–≤, {model_config.n_layer} —Å–ª–æ–µ–≤")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º GPT-2 —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä)
    from transformers import GPT2Tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    logger.info(f"üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {len(tokenizer)} —Ç–æ–∫–µ–Ω–æ–≤")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    model_config.vocab_size = len(tokenizer)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    logger.info("üß† –°–æ–∑–¥–∞—é –º–æ–¥–µ–ª—å...")
    model = create_model_18k(model_config)
    model.to(device)
    
    # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {total_params:,} (–æ–±—É—á–∞–µ–º—ã—Ö: {trainable_params:,})")
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
    monitor_memory_usage()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset_path = "/Users/anton/proj/ai.nativemind.net/sridhar/combined_shridhar_alice_dataset"
    logger.info(f"üìö –ó–∞–≥—Ä—É–∂–∞—é –¥–∞—Ç–∞—Å–µ—Ç –∏–∑: {dataset_path}")
    
    train_dataset = ShridharDataset18K(dataset_path, tokenizer, max_length=model_config.n_positions)
    logger.info(f"üìñ –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(train_dataset)}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    val_dataset = ShridharDataset18K(dataset_path, tokenizer, max_length=model_config.n_positions)
    val_dataset.dataset = val_dataset.dataset['validation']  # –ò—Å–ø–æ–ª—å–∑—É–µ–º validation split
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataCollator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LM, –Ω–µ MLM
        pad_to_multiple_of=8
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    training_args = create_training_arguments_18k()
    logger.info(f"‚öôÔ∏è –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è: batch_size={training_args.per_device_train_batch_size}, "
                f"gradient_accumulation={training_args.gradient_accumulation_steps}")
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
    monitor_memory_usage()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
    )
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏
    def log_memory_usage():
        if torch.backends.mps.is_available():
            logger.info("üíæ MPS –∞–∫—Ç–∏–≤–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GPU M4")
        else:
            logger.info("üíæ CPU —Ä–µ–∂–∏–º")
        monitor_memory_usage()
    
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    logger.info("üéØ –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ —Å 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º...")
    log_memory_usage()
    
    try:
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤
        original_train = trainer.train
        
        def monitored_train(*args, **kwargs):
            result = original_train(*args, **kwargs)
            monitor_memory_usage()
            return result
        
        trainer.train = monitored_train
        
        trainer.train()
        logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        logger.info("üíæ –°–æ—Ö—Ä–∞–Ω—è—é –º–æ–¥–µ–ª—å...")
        trainer.save_model()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        tokenizer_config = {
            "vocab_size": model_config.vocab_size,
            "model_max_length": model_config.n_positions,
            "special_tokens": {
                "pad_token": tokenizer.pad_token,
                "eos_token": tokenizer.eos_token,
                "bos_token": tokenizer.bos_token
            }
        }
        
        with open("./shridhar_18k_model/tokenizer_config.json", 'w', encoding='utf-8') as f:
            json.dump(tokenizer_config, f, ensure_ascii=False, indent=2)
        
        logger.info("üéâ –ú–æ–¥–µ–ª—å 18K —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ./shridhar_18k_model/")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        monitor_memory_usage()
        raise

if __name__ == "__main__":
    main()

