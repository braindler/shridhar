#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞ –º–æ–¥–µ–ª–∏ braindler_full_trained_model
—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º 8K –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö: alpaca_data, mozgach_alice_gift_sql_dataset, 
mozgach_alpaca_gift, shridhar_maharaj_books
"""

import torch
import os
import json
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from datasets import load_dataset, concatenate_datasets
import wandb
from accelerate import Accelerator
import deepspeed
from peft import LoraConfig, get_peft_model, TaskType
import numpy as np

class BraindlerFinetuner:
    def __init__(self, model_name="nativemind/braindler_full_trained_model", max_length=8192):
        self.model_name = model_name
        self.max_length = max_length
        self.tokenizer = None
        self.model = None
        
    def setup_model_and_tokenizer(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        print(f"–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å: {self.model_name}")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è 8K –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        model_kwargs = {
            "torch_dtype": torch.float16,
            "device_map": "auto",
            "trust_remote_code": True
        }
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            **model_kwargs
        )
        
        # –†–∞—Å—à–∏—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ 8K
        self.extend_context_length()
        
        print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {self.max_length}")
        
    def extend_context_length(self):
        """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏ –¥–æ 8K"""
        config = self.model.config
        
        # –ò–∑–º–µ–Ω—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø–æ–∑–∏—Ü–∏–π
        if hasattr(config, 'n_positions'):
            config.n_positions = self.max_length
        elif hasattr(config, 'max_position_embeddings'):
            config.max_position_embeddings = self.max_length
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ RoPE –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if hasattr(config, 'rope_theta'):
            config.rope_theta = 10000.0
        
        print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω –¥–æ {self.max_length} —Ç–æ–∫–µ–Ω–æ–≤")
        
    def load_datasets(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        datasets = []
        
        # –°–ø–∏—Å–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        dataset_configs = [
            "nativemind/alpaca_data",
            "nativemind/mozgach_alice_gift_sql_dataset", 
            "nativemind/mozgach_alpaca_gift",
            "nativemind/shridhar_maharaj_books"
        ]
        
        for dataset_name in dataset_configs:
            try:
                print(f"–ó–∞–≥—Ä—É–∂–∞—é –¥–∞—Ç–∞—Å–µ—Ç: {dataset_name}")
                dataset = load_dataset(dataset_name)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
                if 'train' in dataset:
                    if len(dataset) == 1:  # –¢–æ–ª—å–∫–æ train
                        datasets.append(dataset['train'])
                    else:  # –ï—Å—Ç—å train/validation/test
                        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ä–∞–∑–¥–µ–ª—ã
                        combined = concatenate_datasets([
                            dataset['train'], 
                            dataset.get('validation', dataset['train']),
                            dataset.get('test', dataset['train'])
                        ])
                        datasets.append(combined)
                else:
                    # –ï—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫–ª—é—á
                    first_key = list(dataset.keys())[0]
                    datasets.append(dataset[first_key])
                    
                print(f"‚úÖ {dataset_name}: {len(datasets[-1])} –∑–∞–ø–∏—Å–µ–π")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {dataset_name}: {e}")
                continue
        
        if not datasets:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
        combined_dataset = concatenate_datasets(datasets)
        print(f"üìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(combined_dataset)}")
        
        return combined_dataset
        
    def prepare_dataset_for_training(self, dataset):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        
        def tokenize_function(examples):
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è
            texts = []
            
            for key in examples.keys():
                if isinstance(examples[key][0], str):
                    # –ï—Å–ª–∏ —ç—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø–æ–ª–µ, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ
                    texts.extend(examples[key])
                elif isinstance(examples[key][0], list):
                    # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫, –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö
                    for item in examples[key]:
                        if isinstance(item, list):
                            texts.append(' '.join(item))
                        else:
                            texts.append(str(item))
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å —É—á–µ—Ç–æ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
            return self.tokenizer(
                texts,
                truncation=True,
                padding=False,
                max_length=self.max_length,
                return_tensors=None  # –£–±–∏—Ä–∞–µ–º return_tensors –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            )
        
        print("–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É—é –¥–∞—Ç–∞—Å–µ—Ç...")
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names,
            desc="Tokenizing"
        )
        
        return tokenized_dataset
        
    def setup_lora(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞"""
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["c_attn", "c_proj", "c_fc", "c_proj"]
        )
        
        self.model = get_peft_model(self.model, lora_config)
        print("LoRA –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞")
        
    def train(self):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è W&B (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        try:
            wandb.init(
                project="braindler-8k-finetune",
                name="braindler_8k_experiment",
                config={
                    "model": self.model_name,
                    "max_length": self.max_length,
                    "datasets": [
                        "alpaca_data",
                        "mozgach_alice_gift_sql_dataset", 
                        "mozgach_alpaca_gift",
                        "shridhar_maharaj_books"
                    ]
                }
            )
            print("‚úÖ W&B –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è W&B –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {e}")
            print("–ü—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞...")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        self.setup_model_and_tokenizer()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
        self.setup_lora()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        dataset = self.load_datasets()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        tokenized_dataset = self.prepare_dataset_for_training(dataset)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation
        train_size = int(0.9 * len(tokenized_dataset))
        train_dataset = tokenized_dataset.select(range(train_size))
        eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))
        
        print(f"üìà Train: {len(train_dataset)} –∑–∞–ø–∏—Å–µ–π")
        print(f"üìà Validation: {len(eval_dataset)} –∑–∞–ø–∏—Å–µ–π")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        training_args = TrainingArguments(
            output_dir="./braindler_finetuned_8k",
            overwrite_output_dir=True,
            num_train_epochs=3,
            per_device_train_batch_size=1,
            per_device_eval_batch_size=1,
            gradient_accumulation_steps=8,
            warmup_steps=100,
            learning_rate=5e-5,
            fp16=True,
            logging_steps=10,
            eval_steps=500,
            save_steps=1000,
            eval_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            report_to="none",
            run_name="braindler_8k_finetune",
            remove_unused_columns=False
        )
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
        )
        
        print("üöÄ –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ...")
        trainer.train()
        
        print("üíæ –°–æ—Ö—Ä–∞–Ω—è—é –º–æ–¥–µ–ª—å...")
        trainer.save_model()
        self.tokenizer.save_pretrained("./braindler_finetuned_8k")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã
        self.model.save_pretrained("./braindler_finetuned_8k_lora")
        
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.test_model()
        
    def test_model(self):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        print("üß™ –¢–µ—Å—Ç–∏—Ä—É—é –º–æ–¥–µ–ª—å...")
        
        test_prompts = [
            "–†–∞—Å—Å–∫–∞–∂–∏ –æ —Ñ–∏–ª–æ—Å–æ—Ñ–∏–∏ –≤–∞–π—à–Ω–∞–≤–∏–∑–º–∞:",
            "–ß—Ç–æ —Ç–∞–∫–æ–µ –±—Ö–∞–∫—Ç–∏-–π–æ–≥–∞?",
            "–û–±—ä—è—Å–Ω–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ö—Ä–∏—à–Ω—ã:",
            "–ö–∞–∫–æ–≤–∞ —Ä–æ–ª—å –≥—É—Ä—É –≤ –¥—É—Ö–æ–≤–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–µ?"
        ]
        
        for prompt in test_prompts:
            print(f"\nüìù –ü—Ä–æ–º–ø—Ç: {prompt}")
            
            inputs = self.tokenizer(prompt, return_tensors="pt")
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=200,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"ü§ñ –û—Ç–≤–µ—Ç: {response[len(prompt):]}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ó–∞–ø—É—Å–∫ —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞ Braindler —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º 8K")
    
    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Ñ–∞–π–Ω—Ç—é–Ω–µ—Ä–∞
    finetuner = BraindlerFinetuner(max_length=8192)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    finetuner.train()

if __name__ == "__main__":
    main()
