#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

def test_finetuned_model():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–Ω—Ç—é–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    base_model_name = "nativemind/braindler_full_trained_model"
    lora_path = "./braindler_finetuned_8k_lora"
    
    print("–ó–∞–≥—Ä—É–∂–∞—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    print("–ó–∞–≥—Ä—É–∂–∞—é LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã...")
    model = PeftModel.from_pretrained(base_model, lora_path)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã
    test_prompts = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –±—Ö–∞–∫—Ç–∏-–π–æ–≥–∞?",
        "–û–±—ä—è—Å–Ω–∏ —Ñ–∏–ª–æ—Å–æ—Ñ–∏—é –®—Ä–∏–ª—ã –®—Ä–∏–¥—Ö–∞—Ä–∞ –ú–∞—Ö–∞—Ä–∞–¥–∂–∞:",
        "–ö–∞–∫–æ–≤–∞ —Ä–æ–ª—å –ö—Ä–∏—à–Ω—ã –≤ –≤–∞–π—à–Ω–∞–≤–∏–∑–º–µ?",
        "–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –ø—Ä–µ–¥–∞–Ω–Ω–æ–µ —Å–ª—É–∂–µ–Ω–∏–µ?",
        "–†–∞—Å—Å–∫–∞–∂–∏ –æ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –≥—É—Ä—É –≤ –≤–∞–π—à–Ω–∞–≤–∏–∑–º–µ:",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ä–∞—Å–∞ –≤ —Ñ–∏–ª–æ—Å–æ—Ñ–∏–∏ –≥–∞—É–¥–∏—è-–≤–∞–π—à–Ω–∞–≤–æ–≤?",
        "–û–±—ä—è—Å–Ω–∏ —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –¥–∂–∏–≤–æ–π –∏ –ë—Ä–∞—Ö–º–∞–Ω–æ–º:",
        "–ö–∞–∫–æ–≤–∞ —Ä–æ–ª—å –®—Ä–∏–º–∞—Ç–∏ –†–∞–¥—Ö–∞—Ä–∞–Ω–∏ –≤ –¥—É—Ö–æ–≤–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–µ?"
    ]
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n{'='*60}")
        print(f"üß™ –¢–µ—Å—Ç {i}/8")
        print(f"üìù –ü—Ä–æ–º–ø—Ç: {prompt}")
        print(f"{'='*60}")
        
        inputs = tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"ü§ñ –û—Ç–≤–µ—Ç: {response[len(prompt):]}")
        
        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–µ—Å—Ç–∞–º–∏
        import time
        time.sleep(1)

def test_context_length():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    print("\n" + "="*60)
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (8K —Ç–æ–∫–µ–Ω–æ–≤)")
    print("="*60)
    
    base_model_name = "nativemind/braindler_full_trained_model"
    lora_path = "./braindler_finetuned_8k_lora"
    
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    model = PeftModel.from_pretrained(base_model, lora_path)
    
    # –°–æ–∑–¥–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    long_prompt = """
    –í —Ñ–∏–ª–æ—Å–æ—Ñ–∏–∏ –≥–∞—É–¥–∏—è-–≤–∞–π—à–Ω–∞–≤–æ–≤, –∫–∞–∫ –æ–±—ä—è—Å–Ω—è–µ—Ç –®—Ä–∏–ª–∞ –®—Ä–∏–¥—Ö–∞—Ä –ú–∞—Ö–∞—Ä–∞–¥–∂, 
    —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≥–ª—É–±–æ–∫–∞—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è –±—Ö–∞–∫—Ç–∏-–π–æ–≥–∏, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π 
    –ø—É—Ç—å –ø—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ —Å–ª—É–∂–µ–Ω–∏—è –í—Å–µ–≤—ã—à–Ω–µ–º—É –ì–æ—Å–ø–æ–¥—É. –≠—Ç–∞ —Ñ–∏–ª–æ—Å–æ—Ñ–∏—è –æ—Å–Ω–æ–≤–∞–Ω–∞ 
    –Ω–∞ —É—á–µ–Ω–∏–∏ –®—Ä–∏ –ß–∞–π—Ç–∞–Ω—å–∏ –ú–∞—Ö–∞–ø—Ä–∞–±—Ö—É –∏ –µ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è 
    –®—Ä–∏–ª—É –†—É–ø—É –ì–æ—Å–≤–∞–º–∏, –®—Ä–∏–ª—É –°–∞–Ω–∞—Ç–∞–Ω—É –ì–æ—Å–≤–∞–º–∏ –∏ –¥—Ä—É–≥–∏—Ö –≤–µ–ª–∏–∫–∏—Ö –∞—á–∞—Ä—å–µ–≤.
    
    –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è:
    1. –ö–æ–Ω—Ü–µ–ø—Ü–∏—é –¥–∂–∏–≤—ã –∫–∞–∫ –≤–µ—á–Ω–æ–π —á–∞—Å—Ç–∏—Ü—ã –ë–æ–≥–∞
    2. –†–æ–ª—å –≥—É—Ä—É –≤ –¥—É—Ö–æ–≤–Ω–æ–º —Ä–∞–∑–≤–∏—Ç–∏–∏
    3. –í–∞–∂–Ω–æ—Å—Ç—å —Å–≤—è—Ç–æ–≥–æ –∏–º–µ–Ω–∏
    4. –°–ª—É–∂–µ–Ω–∏–µ –≤ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–∏ –ø—Ä–µ–¥–∞–Ω–Ω–æ—Å—Ç–∏
    5. –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–∞—Å—ã –∏ –±—Ö–∞–≤—ã
    
    –†–∞—Å—Å–∫–∞–∂–∏ –ø–æ–¥—Ä–æ–±–Ω–æ –æ –∫–∞–∂–¥–æ–º –∏–∑ —ç—Ç–∏—Ö –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –∏ –∏—Ö –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏:
    """
    
    print(f"üìù –î–ª–∏–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç ({len(long_prompt)} —Å–∏–º–≤–æ–ª–æ–≤):")
    print(long_prompt[:200] + "...")
    
    inputs = tokenizer(long_prompt, return_tensors="pt")
    print(f"üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ–º–ø—Ç–µ: {inputs['input_ids'].shape[1]}")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=500,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"\nü§ñ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:")
    print(response[len(long_prompt):])

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–Ω—Ç—é–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Braindler —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º 8K")
    print("="*70)
    
    try:
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Å—Ç—ã
        test_finetuned_model()
        
        # –¢–µ—Å—Ç –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        test_context_length()
        
        print("\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.")

if __name__ == "__main__":
    main()
