#!/usr/bin/env python3
"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –®—Ä–∏–¥—Ö–∞—Ä–∞ –ú–∞—Ö–∞—Ä–∞–¥–∂–∞ —Å 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
–° –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –ø–∞–º—è—Ç–∏ –¥–ª—è MacBook Pro M4
"""

import os
import json
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    GPT2Config,
    GPT2LMHeadModel
)
from datasets import load_from_disk
import numpy as np
from typing import Dict, List, Any
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ShridharModelConfig18KOptimized:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self):
        self.vocab_size = 50257  # GPT-2 —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
        self.n_positions = 18432  # 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç
        self.n_embd = 512        # –£–º–µ–Ω—å—à–µ–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        self.n_layer = 8         # –ú–µ–Ω—å—à–µ —Å–ª–æ–µ–≤
        self.n_head = 8          # –ú–µ–Ω—å—à–µ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
        self.attn_pdrop = 0.1
        self.embd_pdrop = 0.1
        self.resid_pdrop = 0.1
        self.activation_function = "gelu_new"
        self.bos_token_id = 0
        self.eos_token_id = 1
        self.pad_token_id = 2

class ShridharDataset18KOptimized:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
    
    def __init__(self, dataset_path: str, tokenizer, max_length: int = 18432):
        self.dataset = load_from_disk(dataset_path)
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.dataset['train'])
    
    def __getitem__(self, idx):
        item = self.dataset['train'][idx]
        text = item['text']
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if item.get('context_type') == 'spiritual_text':
            text = f"<|spiritual|>{text}<|end|>"
        elif item.get('context_type') == 'dialogue':
            text = f"<|dialogue|>{text}<|end|>"
        elif item.get('context_type') == 'script':
            text = f"<|script|>{text}<|end|>"
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å –ø–æ–º–æ—â—å—é GPT-2 —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        tokens = self.tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=self.max_length)
        
        return {
            'input_ids': torch.tensor(tokens, dtype=torch.long),
            'labels': torch.tensor(tokens, dtype=torch.long)
        }

def create_model_18k_optimized(config: ShridharModelConfig18KOptimized) -> GPT2LMHeadModel:
    """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ GPT-2 –¥–ª—è 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    
    model_config = GPT2Config(
        vocab_size=config.vocab_size,
        n_positions=config.n_positions,
        n_embd=config.n_embd,
        n_layer=config.n_layer,
        n_head=config.n_head,
        attn_pdrop=config.attn_pdrop,
        embd_pdrop=config.embd_pdrop,
        resid_pdrop=config.resid_pdrop,
        activation_function=config.activation_function,
        bos_token_id=config.bos_token_id,
        eos_token_id=config.eos_token_id,
        pad_token_id=config.pad_token_id
    )
    
    model = GPT2LMHeadModel(model_config)
    return model

def setup_training_environment_18k_optimized():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ä–µ–¥—ã –æ–±—É—á–µ–Ω–∏—è –¥–ª—è M4 —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ MPS
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'  # –û—Ç–∫–ª—é—á–∞–µ–º –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏ MPS
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å MPS
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è MPS (Metal Performance Shaders)")
        logger.info("üîß MPS –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏ –æ—Ç–∫–ª—é—á–µ–Ω")
    else:
        device = torch.device("cpu")
        logger.warning("‚ö†Ô∏è MPS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
    if device.type == "mps":
        import gc
        gc.collect()
        logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
    
    return device

def create_training_arguments_18k_optimized():
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    
    return TrainingArguments(
        output_dir="./shridhar_18k_optimized_model",
        per_device_train_batch_size=1,        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π batch
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=64,       # –ë–æ–ª—å—à–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        learning_rate=3e-5,                      # –ú–µ–Ω—å—à–∏–π learning rate
        num_train_epochs=2,                   # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö
        max_steps=8000,                       # –ú–µ–Ω—å—à–µ —à–∞–≥–æ–≤
        warmup_steps=500,                    # –ú–µ–Ω—å—à–µ warmup
        logging_steps=500,                    # –†–µ–∂–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        save_steps=2000,                      # –†–µ–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        eval_steps=2000,
        eval_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        fp16=False,                          # MPS –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç fp16
        gradient_checkpointing=True,         # –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        dataloader_pin_memory=False,         # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        dataloader_num_workers=1,            # –ú–∏–Ω–∏–º—É–º –≤–æ—Ä–∫–µ—Ä–æ–≤
        remove_unused_columns=False,
        report_to="tensorboard",
        run_name="shridhar_18k_optimized_training",
        save_total_limit=1,                 # –¢–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
        prediction_loss_only=True,
        dataloader_drop_last=True,
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è 18K
        dataloader_prefetch_factor=1,        # –ú–∏–Ω–∏–º—É–º prefetch
        max_grad_norm=0.5,                  # –°—Ç—Ä–æ–≥–∏–π gradient clipping
        weight_decay=0.1,                   # –ë–æ–ª—å—à–µ weight decay
        adam_beta1=0.9,
        adam_beta2=0.999,
        adam_epsilon=1e-8,
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        dataloader_persistent_workers=False,  # –û—Ç–∫–ª—é—á–∞–µ–º persistent workers
        dataloader_multiprocessing_context=None,  # –û—Ç–∫–ª—é—á–∞–µ–º multiprocessing
    )

def monitor_memory_usage():
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    import psutil
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏
    memory = psutil.virtual_memory()
    logger.info(f"üíæ –ü–∞–º—è—Ç—å: {memory.used / (1024**3):.1f}GB / {memory.total / (1024**3):.1f}GB ({memory.percent:.1f}%)")
    
    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
    if memory.percent > 95:
        logger.error("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏! –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è.")
        return False
    elif memory.percent > 90:
        logger.warning("‚ö†Ô∏è –û—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏!")
    elif memory.percent > 80:
        logger.warning("‚ö†Ô∏è –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏.")
    else:
        logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ –Ω–æ—Ä–º–µ.")
    
    return True

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
    
    logger.info("üöÄ –ù–∞—á–∏–Ω–∞—é –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –®—Ä–∏–¥—Ö–∞—Ä–∞ –ú–∞—Ö–∞—Ä–∞–¥–∂–∞ —Å 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º...")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ä–µ–¥—ã
    device = setup_training_environment_18k_optimized()
    logger.info(f"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏
    if not monitor_memory_usage():
        logger.error("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –û—Å—Ç–∞–Ω–æ–≤–∫–∞.")
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
    model_config = ShridharModelConfig18KOptimized()
    logger.info(f"üìê –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {model_config.n_positions} —Ç–æ–∫–µ–Ω–æ–≤, {model_config.n_layer} —Å–ª–æ–µ–≤, {model_config.n_embd} —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    from transformers import GPT2Tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    logger.info(f"üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {len(tokenizer)} —Ç–æ–∫–µ–Ω–æ–≤")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    model_config.vocab_size = len(tokenizer)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    logger.info("üß† –°–æ–∑–¥–∞—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
    model = create_model_18k_optimized(model_config)
    model.to(device)
    
    # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {total_params:,} (–æ–±—É—á–∞–µ–º—ã—Ö: {trainable_params:,})")
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
    if not monitor_memory_usage():
        logger.error("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏. –û—Å—Ç–∞–Ω–æ–≤–∫–∞.")
        return
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset_path = "/Users/anton/proj/ai.nativemind.net/sridhar/combined_shridhar_alice_dataset"
    logger.info(f"üìö –ó–∞–≥—Ä—É–∂–∞—é –¥–∞—Ç–∞—Å–µ—Ç –∏–∑: {dataset_path}")
    
    train_dataset = ShridharDataset18KOptimized(dataset_path, tokenizer, max_length=model_config.n_positions)
    logger.info(f"üìñ –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(train_dataset)}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    val_dataset = ShridharDataset18KOptimized(dataset_path, tokenizer, max_length=model_config.n_positions)
    val_dataset.dataset = val_dataset.dataset['validation']
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataCollator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    training_args = create_training_arguments_18k_optimized()
    logger.info(f"‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã: batch_size={training_args.per_device_train_batch_size}, "
                f"gradient_accumulation={training_args.gradient_accumulation_steps}")
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
    if not monitor_memory_usage():
        logger.error("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º. –û—Å—Ç–∞–Ω–æ–≤–∫–∞.")
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
    )
    
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    logger.info("üéØ –ù–∞—á–∏–Ω–∞—é –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –æ–±—É—á–µ–Ω–∏–µ —Å 18K –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º...")
    
    try:
        trainer.train()
        logger.info("‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        logger.info("üíæ –°–æ—Ö—Ä–∞–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
        trainer.save_model()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        tokenizer_config = {
            "vocab_size": model_config.vocab_size,
            "model_max_length": model_config.n_positions,
            "special_tokens": {
                "pad_token": tokenizer.pad_token,
                "eos_token": tokenizer.eos_token,
                "bos_token": tokenizer.bos_token
            }
        }
        
        with open("./shridhar_18k_optimized_model/tokenizer_config.json", 'w', encoding='utf-8') as f:
            json.dump(tokenizer_config, f, ensure_ascii=False, indent=2)
        
        logger.info("üéâ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å 18K —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ./shridhar_18k_optimized_model/")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: {e}")
        monitor_memory_usage()
        raise

if __name__ == "__main__":
    main()

