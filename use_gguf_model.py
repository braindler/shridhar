#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GGUF –º–æ–¥–µ–ª–∏ shridhar_8k
"""

from llama_cpp import Llama

def load_gguf_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ GGUF –º–æ–¥–µ–ª–∏"""
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞—é GGUF –º–æ–¥–µ–ª—å...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    llm = Llama(
        model_path="./shridhar_8k.gguf",
        n_ctx=8192,  # 8K –∫–æ–Ω—Ç–µ–∫—Å—Ç
        n_threads=4,
        verbose=False
    )
    
    print("‚úÖ GGUF –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    return llm

def generate_text(llm, prompt, max_tokens=300):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é GGUF –º–æ–¥–µ–ª–∏"""
    print(f"üìù –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞: {prompt}")
    
    response = llm(
        prompt,
        max_tokens=max_tokens,
        temperature=0.7,
        top_p=0.9,
        repeat_penalty=1.1,
        stop=["</s>", "

"]
    )
    
    return response['choices'][0]['text']

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GGUF –º–æ–¥–µ–ª–∏ shridhar_8k")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    llm = load_gguf_model()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã
    test_prompts = [
        "–†–∞—Å—Å–∫–∞–∂–∏ –æ —Ñ–∏–ª–æ—Å–æ—Ñ–∏–∏ –≤–∞–π—à–Ω–∞–≤–∏–∑–º–∞:",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –±—Ö–∞–∫—Ç–∏-–π–æ–≥–∞?",
        "–û–±—ä—è—Å–Ω–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ö—Ä–∏—à–Ω—ã:",
        "–ö–∞–∫–æ–≤–∞ —Ä–æ–ª—å –≥—É—Ä—É –≤ –¥—É—Ö–æ–≤–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–µ?"
    ]
    
    for prompt in test_prompts:
        print(f"\n{'='*60}")
        print(f"üìù –ü—Ä–æ–º–ø—Ç: {prompt}")
        print(f"{'='*60}")
        
        response = generate_text(llm, prompt)
        print(f"ü§ñ –û—Ç–≤–µ—Ç: {response}")

if __name__ == "__main__":
    main()
