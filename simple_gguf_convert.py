#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ GGUF —Ñ–æ—Ä–º–∞—Ç
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import subprocess

def convert_to_gguf():
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ GGUF —Ñ–æ—Ä–º–∞—Ç"""
    print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é –º–æ–¥–µ–ª—å shridhar_8k –≤ GGUF...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    base_model_name = "nativemind/braindler_full_trained_model"
    lora_path = "./braindler_finetuned_8k"
    
    print("–ó–∞–≥—Ä—É–∂–∞—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="cpu"
    )
    
    print("–ó–∞–≥—Ä—É–∂–∞—é LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã...")
    model = PeftModel.from_pretrained(base_model, lora_path)
    
    print("–û–±—ä–µ–¥–∏–Ω—è—é LoRA —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é...")
    merged_model = model.merge_and_unload()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    output_dir = "./shridhar_8k_merged"
    print(f"–°–æ—Ö—Ä–∞–Ω—è—é –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ {output_dir}...")
    merged_model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è GGUF
    config = {
        "model_type": "gpt2",
        "vocab_size": merged_model.config.vocab_size,
        "n_positions": 8192,  # 8K –∫–æ–Ω—Ç–µ–∫—Å—Ç
        "n_embd": merged_model.config.n_embd,
        "n_layer": merged_model.config.n_layer,
        "n_head": merged_model.config.n_head,
        "activation_function": merged_model.config.activation_function,
        "bos_token_id": merged_model.config.bos_token_id,
        "eos_token_id": merged_model.config.eos_token_id,
        "pad_token_id": merged_model.config.pad_token_id
    }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    import json
    with open(f"{output_dir}/gguf_config.json", "w") as f:
        json.dump(config, f, indent=2)
    
    print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ GGUF")
    print(f"üìÅ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_dir}")
    print("üìã –î–ª—è –ø–æ–ª–Ω–æ–π –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ GGUF –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ llama.cpp:")
    print("   git clone https://github.com/ggerganov/llama.cpp.git")
    print("   cd llama.cpp && make")
    print("   python convert_hf_to_gguf.py ../shridhar_8k_merged --outfile ../shridhar_8k.gguf --outtype f16")
    
    return output_dir

def create_usage_example():
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GGUF –º–æ–¥–µ–ª–∏"""
    usage_example = '''#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GGUF –º–æ–¥–µ–ª–∏ shridhar_8k
"""

from llama_cpp import Llama

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GGUF –º–æ–¥–µ–ª–∏ shridhar_8k")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º GGUF –º–æ–¥–µ–ª—å
    print("–ó–∞–≥—Ä—É–∂–∞—é GGUF –º–æ–¥–µ–ª—å...")
    llm = Llama(
        model_path="./shridhar_8k.gguf",
        n_ctx=8192,  # 8K –∫–æ–Ω—Ç–µ–∫—Å—Ç
        n_threads=4,
        verbose=False
    )
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã
    test_prompts = [
        "–†–∞—Å—Å–∫–∞–∂–∏ –æ —Ñ–∏–ª–æ—Å–æ—Ñ–∏–∏ –≤–∞–π—à–Ω–∞–≤–∏–∑–º–∞:",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –±—Ö–∞–∫—Ç–∏-–π–æ–≥–∞?",
        "–û–±—ä—è—Å–Ω–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ö—Ä–∏—à–Ω—ã:",
        "–ö–∞–∫–æ–≤–∞ —Ä–æ–ª—å –≥—É—Ä—É –≤ –¥—É—Ö–æ–≤–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–µ?"
    ]
    
    for prompt in test_prompts:
        print(f"\\n{'='*60}")
        print(f"üìù –ü—Ä–æ–º–ø—Ç: {prompt}")
        print(f"{'='*60}")
        
        response = llm(
            prompt,
            max_tokens=300,
            temperature=0.7,
            top_p=0.9,
            repeat_penalty=1.1,
            stop=["</s>", "\\n\\n"]
        )
        
        print(f"ü§ñ –û—Ç–≤–µ—Ç: {response['choices'][0]['text']}")

if __name__ == "__main__":
    main()
'''
    
    with open("./use_gguf_example.py", "w") as f:
        f.write(usage_example)
    
    print("‚úÖ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GGUF —Å–æ–∑–¥–∞–Ω: use_gguf_example.py")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ shridhar_8k –¥–ª—è GGUF –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
    model_path = convert_to_gguf()
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    create_usage_example()
    
    print("\\nüìã –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    print("1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ llama.cpp: git clone https://github.com/ggerganov/llama.cpp.git")
    print("2. –°–∫–æ–º–ø–∏–ª–∏—Ä—É–π—Ç–µ: cd llama.cpp && make")
    print("3. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–π—Ç–µ: python convert_hf_to_gguf.py ../shridhar_8k_merged --outfile ../shridhar_8k.gguf --outtype f16")
    print("4. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: python use_gguf_example.py")
    
    print("\\n‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ GGUF!")

if __name__ == "__main__":
    main()
